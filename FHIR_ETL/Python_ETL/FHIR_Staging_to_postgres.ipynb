{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08512380",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 117/117 [16:28<00:00,  8.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totals by resourceType:\n",
      "Practitioner: 0\n",
      "PractitionerRole: 0\n",
      "Patient: 5834\n",
      "Encounter: 350515\n",
      "Observation: 3065994\n",
      "Condition: 214665\n",
      "Claim: 653713\n",
      "DiagnosticReport: 696976\n",
      "DocumentReference: 350515\n",
      "ExplanationOfBenefit: 653713\n",
      "CarePlan: 19438\n",
      "Immunization: 83349\n",
      "Device: 33816\n",
      "SupplyDelivery: 154592\n",
      "Medication: 106672\n",
      "MedicationRequest: 303198\n",
      "MedicationAdministration: 106672\n",
      "ImagingStudy: 27895\n",
      "Procedure: 967105\n",
      "Organization: 0\n",
      "Provenance: 5834\n",
      "CareTeam: 19438\n",
      "AllergyIntolerance: 5163\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#--------------------------09/10/2025-------------------------#\n",
    "#--------------------------Patients---------------------------#\n",
    "# runs 218 min 21.5 sec\n",
    "import os\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import logging\n",
    "import psycopg2\n",
    "from psycopg2.extras import Json, execute_values\n",
    "import orjson  # add this at the top of your file\n",
    "import csv\n",
    "import time\n",
    "import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "#from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import shutil\n",
    "import collections\n",
    "from collections import Counter\n",
    "from collections import defaultdict\n",
    "\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    \"\"\"Create a dedicated logger for a parser method.\"\"\"\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    \n",
    "    # Avoid adding multiple handlers if logger already exists\n",
    "    if not logger.handlers:\n",
    "        file_handler = logging.FileHandler(log_file, mode='w', encoding='utf-8')\n",
    "        file_handler.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        file_handler.setFormatter(formatter)\n",
    "        logger.addHandler(file_handler)\n",
    "    \n",
    "    return logger\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# DB config\n",
    "# -----------------------------\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"FHIR_staging\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"new_password\"\n",
    "}\n",
    "\n",
    "# -----------------------------\n",
    "# FHIR staging map\n",
    "# -----------------------------\n",
    "\n",
    "# FHIR resource → staging table mapping\n",
    "FHIR_STAGING_MAP = {\n",
    "    \"Practitioner\": (\"practitioners_fhir_raw\", \"practitioner_id\"),\n",
    "    \"PractitionerRole\": (\"practitioner_roles_fhir_raw\", \"practitioner_role_id\"),\n",
    "    \"Patient\": (\"patients_fhir_raw\", \"patients_id\"),\n",
    "    \"Encounter\": (\"encounters_fhir_raw\", \"encounter_id\"),\n",
    "    \"Observation\": (\"observations_fhir_raw\", \"observation_id\"),\n",
    "    \"Condition\": (\"conditions_fhir_raw\", \"condition_id\"),\n",
    "    \"Claim\": (\"claims_fhir_raw\", \"claim_id\"),\n",
    "    \"DiagnosticReport\": (\"diagnostics_fhir_raw\", \"diagnostic_id\"),\n",
    "    \"DocumentReference\": (\"document_references_fhir_raw\", \"document_reference_id\"),\n",
    "    \"ExplanationOfBenefit\": (\"explanationofbenefits_fhir_raw\", \"explanationofbenefit_id\"),\n",
    "    \"CarePlan\": (\"careplans_fhir_raw\", \"careplan_id\"),\n",
    "    \"Immunization\": (\"immunizations_fhir_raw\", \"immunization_id\"),\n",
    "    \"Device\": (\"devices_fhir_raw\", \"device_id\"),\n",
    "    \"SupplyDelivery\": (\"supplydeliveries_fhir_raw\", \"supplydelivery_id\"),\n",
    "    \"Medication\": (\"medications_fhir_raw\", \"medication_id\"),\n",
    "    \"MedicationRequest\": (\"medicationrequests_fhir_raw\", \"medicationrequest_id\"),\n",
    "    \"MedicationAdministration\": (\"medicationadministrations_fhir_raw\", \"medicationadministration_id\"),\n",
    "    \"ImagingStudy\": (\"imagingstudies_fhir_raw\", \"imagingstudy_id\"),\n",
    "    \"Procedure\": (\"procedures_fhir_raw\", \"procedure_id\"),\n",
    "    \"Organization\": (\"organizations_fhir_raw\", \"organization_id\"),\n",
    "    \"Provenance\":(\"provenances_fhir_raw\", \"provenance_id\"),\n",
    "    \"CareTeam\":(\"careteams_fhir_raw\", \"careteam_id\"),\n",
    "    \"AllergyIntolerance\":(\"allergyintolerances_fhir_raw\", \"allergyintolerance_id\")\n",
    "}\n",
    "\n",
    "global_counters = {k: 0 for k in FHIR_STAGING_MAP.keys()}\n",
    "\n",
    "#-----------------------------\n",
    "# Chunk file helper\n",
    "#-----------------------------\n",
    "\n",
    "def chunk_files(file_list, chunk_size):\n",
    "    \"\"\"Split list of files into chunks of given size.\"\"\"\n",
    "    for i in range(0, len(file_list), chunk_size):\n",
    "        yield file_list[i:i + chunk_size]\n",
    "\n",
    "#---------------------------------------------------\n",
    "# Process multiples in one thread\n",
    "#----------------------------------------------------\n",
    "def process_file_chunk(file_chunk):\n",
    "    \"\"\"Process a chunk of files using a single DB connection per thread.\"\"\"\n",
    "    chunk_counters = {k: 0 for k in FHIR_STAGING_MAP.keys()}\n",
    "    # Open one connection for this worker\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    try:\n",
    "        for file_path in file_chunk:\n",
    "            try:\n",
    "                result = process_file(file_path,  conn=conn)\n",
    "                if result:\n",
    "                    for k, v in result.items():\n",
    "                        chunk_counters[k] += v\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing file {file_path}: {e}\")\n",
    "                conn.rollback()  # rollback on file-level errors\n",
    "    finally:\n",
    "        conn.close()\n",
    "    return chunk_counters\n",
    "\n",
    "\n",
    "#import io\n",
    "\n",
    "def insert_batch_copy(conn, table, id_field, rows):\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    buffer = io.StringIO()\n",
    "    writer = csv.writer(buffer, quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "    for rid, resource_json in rows:\n",
    "        #json_text = orjson.dumps(resource_json).decode('utf-8')\n",
    "        #writer.writerow([rid, json_text])\n",
    "        writer.writerow([rid, orjson.dumps(resource_json).decode()])\n",
    "    buffer.seek(0)\n",
    "\n",
    "    #sql = f\"COPY fhir_staging.{table} ({id_field}, resource) FROM STDIN WITH (FORMAT text, DELIMITER E'\\t')\"\n",
    "    #sql = f\"COPY fhir_staging.{table} ({id_field}, resource) FROM STDIN WITH CSV\"\n",
    "    sql = f\"COPY fhir_staging_sample.{table} ({id_field}, resource) FROM STDIN WITH CSV\"\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            cur.copy_expert(sql, buffer)\n",
    "        conn.commit()\n",
    "        return len(rows)\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        logger.error(f\"COPY error for {table}: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# Insert batch helper\n",
    "# -----------------------------\n",
    "def insert_batch(conn, table, id_field, rows):\n",
    "    if not rows:\n",
    "        return 0\n",
    "    #sql = f\"\"\"\n",
    "    #    INSERT INTO fhir_staging.{table} ({id_field}, resource)\n",
    "    #    VALUES %s\n",
    "    #    ON CONFLICT ({id_field}) DO NOTHING\n",
    "    #\"\"\"\n",
    "\n",
    "    sql = f\"\"\"\n",
    "        INSERT INTO fhir_staging_sample.{table} ({id_field}, resource)\n",
    "        VALUES %s\n",
    "        ON CONFLICT ({id_field}) DO NOTHING\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            execute_values(cur, sql, rows)\n",
    "        #conn.commit()\n",
    "        return len(rows)\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        logger.error(f\"Error inserting batch into {table}: {e}\")\n",
    "        return 0\n",
    "\n",
    "# -----------------------------\n",
    "# Process one JSON file\n",
    "# -----------------------------\n",
    "def process_file(file_path,  conn=None):\n",
    "    all_rows = []\n",
    "    inserted_summary = {k:0 for k in FHIR_STAGING_MAP.keys()}\n",
    "\n",
    "    close_conn = False\n",
    "    if conn is None:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        close_conn = True\n",
    "    try:\n",
    "        #conn = psycopg2.connect(**DB_CONFIG)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        resources = data.get(\"entry\", [])\n",
    "        batch_map = {}\n",
    "\n",
    "        resource_counter = collections.Counter()\n",
    "        batch_map = defaultdict(list)\n",
    "        for entry in resources:\n",
    "            resource = entry.get(\"resource\")\n",
    "            if not resource:\n",
    "                continue\n",
    "\n",
    "            rtype = resource.get(\"resourceType\")\n",
    "            rid = resource.get(\"id\")\n",
    "\n",
    "\n",
    "            if not rtype or not rid:\n",
    "                continue\n",
    "            \n",
    "            if rtype not in FHIR_STAGING_MAP:\n",
    "                logger.warning(f\"Skipping unsupported resourceType: {rtype}\")\n",
    "                continue\n",
    "\n",
    "            table, id_field = FHIR_STAGING_MAP[rtype]\n",
    "            all_rows.append((table, id_field, rid, resource))\n",
    "\n",
    "        for table, id_field, rid, resource_json in all_rows:\n",
    "            batch_map[(table, id_field)].append((rid, resource_json))\n",
    "\n",
    "        # Insert per table\n",
    "        for (table, id_field), rows in batch_map.items():\n",
    "            inserted = insert_batch_copy(conn, table, id_field, rows)\n",
    "            rtype = next((k for k, v in FHIR_STAGING_MAP.items() if v[0] == table), table)\n",
    "            inserted_summary[rtype] += inserted\n",
    "\n",
    "        conn.commit()  # commit once per file\n",
    "\n",
    "        logger.info(f\"Processed {os.path.basename(file_path)}: {inserted_summary}\")\n",
    "        return inserted_summary\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to process {file_path}: {e}\")\n",
    "        if conn:\n",
    "            conn.rollback()\n",
    "        return None\n",
    "    finally:\n",
    "        if close_conn:\n",
    "            conn.close()\n",
    "\n",
    "def process_file_for_batch(file_path):\n",
    "    print(f\"Processing {file_path}\")\n",
    "    batch_map = defaultdict(list)\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "        #batch_map = {}    \n",
    "        for entry in data.get(\"entry\", []):\n",
    "            resource = entry.get(\"resource\")\n",
    "            if not resource:\n",
    "                continue\n",
    "            rtype = resource.get(\"resourceType\")\n",
    "            rid = resource.get(\"id\")\n",
    "            if not rtype or not rid or rtype not in FHIR_STAGING_MAP:\n",
    "                continue\n",
    "            table, id_field = FHIR_STAGING_MAP[rtype]\n",
    "            key = (table, id_field)\n",
    "            #batch_map.setdefault(key, []).append((rid, resource))\n",
    "            batch_map[(table, id_field)].append((rid, resource))\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to parse {file_path}: {e}\")\n",
    "        return {}\n",
    "    #return batch_map\n",
    "    return dict(batch_map)\n",
    "\n",
    "# -----------------------------\n",
    "# Folder processing with threads\n",
    "# -----------------------------\n",
    "CHECKPOINT_FILE = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\processed_files.json\"\n",
    "\n",
    "def load_checkpoint():\n",
    "    \"\"\"Load the set of processed files safely.\"\"\"\n",
    "    if os.path.exists(CHECKPOINT_FILE):\n",
    "        try:\n",
    "            with open(CHECKPOINT_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "                data = json.load(f)\n",
    "                if isinstance(data, list):\n",
    "                    return set(data)\n",
    "                else:\n",
    "                    # Corrupted format: ignore and start fresh\n",
    "                    logger.warning(f\"{CHECKPOINT_FILE} is not a list. Starting fresh.\")\n",
    "                    return set()\n",
    "        except json.JSONDecodeError:\n",
    "            # Corrupted JSON: ignore and start fresh\n",
    "            logger.warning(f\"{CHECKPOINT_FILE} is corrupted. Starting fresh.\")\n",
    "            return set()\n",
    "    return set()\n",
    "\n",
    "def save_checkpoint(processed_files):\n",
    "    \"\"\"Save the set of processed files safely.\"\"\"\n",
    "    temp_file = tempfile.NamedTemporaryFile(delete=False, mode=\"w\", encoding=\"utf-8\")\n",
    "    try:\n",
    "        json.dump(list(processed_files), temp_file, indent=2)\n",
    "        temp_file.close()\n",
    "        # Replace old checkpoint atomically\n",
    "        shutil.move(temp_file.name, CHECKPOINT_FILE)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to save checkpoint: {e}\")\n",
    "        if os.path.exists(temp_file.name):\n",
    "            os.remove(temp_file.name)\n",
    "\n",
    "#def process_folder(folder_path, max_workers=4, files_per_worker=50):\n",
    "def process_folder(folder_path, max_workers=8, files_per_worker=50, checkpoint_interval=5):\n",
    "    files = [os.path.abspath(f) for f in glob.glob(os.path.join(folder_path, \"*.json\"))]\n",
    "    if not files:\n",
    "        logger.info(\"No files found.\")\n",
    "        return\n",
    "\n",
    "    processed_files = set(os.path.abspath(f) for f in load_checkpoint())\n",
    "    remaining_files = [f for f in files if f not in processed_files]\n",
    "\n",
    "    logger.info(f\"Starting processing {len(remaining_files)} files...\")\n",
    "\n",
    "    file_chunks = list(chunk_files(remaining_files, files_per_worker))\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = {executor.submit(process_file_chunk, chunk): chunk for chunk in file_chunks}\n",
    "\n",
    "        for future in tqdm(as_completed(futures), total=len(file_chunks)):\n",
    "            chunk = futures[future]\n",
    "            try:\n",
    "                res = future.result()\n",
    "                if res:\n",
    "                    for k, v in res.items():\n",
    "                        global_counters[k] += v\n",
    "\n",
    "                    for f in chunk:\n",
    "                        processed_files.add(f)\n",
    "                    save_checkpoint(processed_files)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error processing chunk {chunk}: {e}\")\n",
    "\n",
    "    logger.info(\"Folder processing completed.\")\n",
    "    logger.info(\"All files processed. Totals by resourceType:\")\n",
    "    for rtype, count in global_counters.items():\n",
    "        logger.info(f\"{rtype}: {count}\")\n",
    "\n",
    "    print(\"Totals by resourceType:\")\n",
    "    for rtype, count in global_counters.items():\n",
    "        print(f\"{rtype}: {count}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Updated process_folder\n",
    "# -----------------------------\n",
    "'''def process_folder(folder_path, max_workers=1):\n",
    "    \n",
    "    files = [os.path.join(folder_path, f) for f in os.listdir(folder_path) if f.endswith(\".json\")]\n",
    "    \n",
    "    if not files:\n",
    "        print(\"No files found.\")\n",
    "        return\n",
    "\n",
    "    processed_files = set(os.path.abspath(f) for f in load_checkpoint())\n",
    "    remaining_files = [os.path.abspath(f) for f in files if os.path.abspath(f) not in processed_files]\n",
    "\n",
    "    #logger.info(f\"Starting processing {len(remaining_files)} files...\")\n",
    "\n",
    "    all_batches = []\n",
    "\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        \n",
    "        futures = {executor.submit(process_file_for_batch, f): f for f in files}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try: \n",
    "                batch_map = future.result()\n",
    "                if batch_map:\n",
    "                    all_batches.append(batch_map)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing a file: {e}\")\n",
    "            processed_files.add(futures[future])\n",
    "            save_checkpoint(processed_files)\n",
    "\n",
    "    # Insert batches in main thread using your insert_batch_copy\n",
    "    conn = psycopg2.connect(**DB_CONFIG)\n",
    "    try:\n",
    "        global_counters = {k: 0 for k in FHIR_STAGING_MAP.keys()}\n",
    "        for batch_map in all_batches:\n",
    "            for (table, id_field), rows in batch_map.items():\n",
    "               \n",
    "                inserted = insert_batch_copy(conn, table, id_field, rows)\n",
    "                rtype = next((k for k, v in FHIR_STAGING_MAP.items() if v[0] == table), table)\n",
    "                global_counters[rtype] += inserted\n",
    "        #conn.commit()\n",
    "    finally:\n",
    "        conn.close()\n",
    "\n",
    "    \n",
    "    print(\"Totals by resourceType:\")\n",
    "    for rtype, count in global_counters.items():\n",
    "        print(f\"{rtype}: {count}\")'''\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    #folder = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Patients\"\n",
    "    #process_folder(folder, max_workers=1)\n",
    "    folder = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Batch_1\\metadata\\Patients\"\n",
    "    process_folder(folder, max_workers=8, files_per_worker=50, checkpoint_interval=5)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    logger = logger(\n",
    "        \"staging_test\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\staging_logger.log\"\n",
    "    )\n",
    "    #folder = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Test_Files\"\n",
    "    #C:\\Users\\tonim\\Downloads\\output\\fhir\\Test_Files\n",
    "    #process_folder(folder, max_workers=8)\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "46623aba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#-------09/10/2025--------------------------#\n",
    "#       Organizations-----------------------#\n",
    "# 4.9 sec to run\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "from psycopg2.extras import execute_batch\n",
    "import pandas as pd\n",
    "\n",
    "# ----------------------------\n",
    "# Logger setup\n",
    "# ----------------------------\n",
    "#staging_logger = logging.getLogger(\"staging_logger\")\n",
    "#logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(message)s\")\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        fh = logging.FileHandler(log_file, 'w', 'utf-8')\n",
    "        fh.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "staging_logger = logger(\n",
    "        \"staging_test\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\staging_logger.log\"\n",
    ")\n",
    "#staging_logger = logging.getLogger(\"staging\")\n",
    "\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        fh = logging.FileHandler(log_file, 'w', 'utf-8')\n",
    "        fh.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# DB Connection\n",
    "# ----------------------------\n",
    "def get_db_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        dbname=\"FHIR_staging\",\n",
    "        user=\"postgres\",\n",
    "        password=\"new_password\",\n",
    "        port=5432\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Batch insert\n",
    "# ----------------------------\n",
    "#df = pd.DataFrame()\n",
    "rows = []\n",
    "id_file_map = pd.DataFrame()\n",
    "def insert_organization_batch(batch, file_path):\n",
    "    if not batch:\n",
    "        return\n",
    "\n",
    "    conn = get_db_connection()\n",
    "    cur = conn.cursor()\n",
    "\n",
    "    #sql = \"\"\"\n",
    "    #    INSERT INTO fhir_staging.organizations_fhir_raw (organization_id, resource)\n",
    "    #    VALUES (%s, %s)\n",
    "    #    ON CONFLICT (organization_id) DO NOTHING\n",
    "    #\"\"\"\n",
    "\n",
    "    sql = \"\"\"\n",
    "        INSERT INTO fhir_staging_sample.organizations_fhir_raw (organization_id, resource)\n",
    "        VALUES (%s, %s)\n",
    "        ON CONFLICT (organization_id) DO NOTHING\n",
    "    \"\"\"\n",
    "\n",
    "    '''sql = f\"\"\"\n",
    "        INSERT INTO fhir_staging.{table} ({id_field}, resource)\n",
    "        VALUES %s\n",
    "        ON CONFLICT ({id_field}) DO NOTHING\n",
    "    \"\"\"'''\n",
    "    \n",
    "    values = [(res.get(\"id\"), json.dumps(res)) for res in batch]\n",
    "    for resource in batch:\n",
    "        resource_id = resource.get(\"id\")\n",
    "        if resource_id: \n",
    "            rows.append({\"file\": os.path.basename(file_path), \"id\": resource_id})\n",
    "            staging_logger.info(f\"resource.getid: {resource_id}\")\n",
    "    \n",
    "\n",
    "    try:\n",
    "        execute_batch(cur, sql, values, page_size=100)\n",
    "        conn.commit()\n",
    "        staging_logger.info(f\"Inserted {len(values)} organization resources into DB\")\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        staging_logger.error(f\"DB insert failed: {e}\")\n",
    "    finally:\n",
    "        cur.close()\n",
    "        conn.close()\n",
    "\n",
    "# ----------------------------\n",
    "# Process a single hospital file\n",
    "# ----------------------------\n",
    "def process_hospital_file(file_path, batch_size=100):\n",
    "    count = 0\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if \"entry\" not in data:\n",
    "            staging_logger.warning(f\"No entries in {file_path}\")\n",
    "            return 0\n",
    "\n",
    "        batch = []\n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\")\n",
    "            if not resource:\n",
    "                continue\n",
    "\n",
    "            batch.append(resource)\n",
    "            count += 1\n",
    "\n",
    "            if len(batch) >= batch_size:\n",
    "                insert_organization_batch(batch, file_path)\n",
    "                batch = []\n",
    "\n",
    "        # Insert any leftover batch\n",
    "        if batch:\n",
    "            insert_organization_batch(batch, file_path)\n",
    "\n",
    "        staging_logger.info(f\"Processed {count} resources from {file_path}\")\n",
    "        return count\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        staging_logger.error(f\"Malformed JSON in {file_path}: {e}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        staging_logger.exception(f\"Error processing file {file_path}\")\n",
    "        return 0\n",
    "\n",
    "# ----------------------------\n",
    "# Sequential loader for all Hospital files\n",
    "# ----------------------------\n",
    "def load_hospital_files_sequential(hospital_folder):\n",
    "    files = glob.glob(os.path.join(hospital_folder, \"*.json\"))\n",
    "    if not files:\n",
    "        staging_logger.warning(f\"No Hospital JSON files found in {hospital_folder}\")\n",
    "        return\n",
    "\n",
    "    total_resources = 0\n",
    "    for file_path in files:\n",
    "        count = process_hospital_file(file_path, batch_size=100)\n",
    "        total_resources += count\n",
    "\n",
    "    staging_logger.info(f\"Finished processing Hospital files. Total resources: {total_resources}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    fhir_folder = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\"\n",
    "    hospital_folder = os.path.join(fhir_folder, \"Hospital\")\n",
    "    load_hospital_files_sequential(hospital_folder)\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[[\"file\", \"id\"]].to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\org_id_1_file.csv\", index=False)\n",
    "    id_counts = df.groupby(\"id\").size().reset_index(name=\"count\")\n",
    "    id_counts.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\count_ids_1_file.csv\")\n",
    "    id_file_map = df.groupby(\"id\")[\"file\"].agg(list).reset_index()\n",
    "    id_file_map.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\total_ids_1_file.csv\")\n",
    "    \n",
    "    #file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Hospital\\hospitalInformation1755716738658.json\"\n",
    "    #load_hospital_files_sequential(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d6c2945",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------09/10/2025--------------------------#\n",
    "#---------------Practitioners-----------------------#\n",
    "# .9 sec to run\n",
    "import os\n",
    "import glob\n",
    "import json\n",
    "import logging\n",
    "import psycopg2\n",
    "from datetime import datetime\n",
    "from psycopg2.extras import execute_batch\n",
    "from psycopg2.extras import Json, execute_values\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# ----------------------------\n",
    "# Logger setup\n",
    "# ----------------------------\n",
    "#staging_logger = logging.getLogger(\"staging_logger\")\n",
    "#logging.basicConfig(level=logging.INFO, format=\"%(levelname)s:%(message)s\")\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        fh = logging.FileHandler(log_file, 'w', 'utf-8')\n",
    "        fh.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "staging_logger = logger(\n",
    "        \"staging_test\", \n",
    "        \"C:\\\\Users\\\\tonim\\\\Downloads\\\\output\\\\fhir\\\\staging_logger.log\"\n",
    ")\n",
    "#staging_logger = logging.getLogger(\"staging\")\n",
    "\n",
    "def logger(name, log_file, level=logging.DEBUG):\n",
    "    logger = logging.getLogger(name)\n",
    "    logger.setLevel(level)\n",
    "    if not logger.handlers:\n",
    "        fh = logging.FileHandler(log_file, 'w', 'utf-8')\n",
    "        fh.setLevel(level)\n",
    "        formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
    "        fh.setFormatter(formatter)\n",
    "        logger.addHandler(fh)\n",
    "    return logger\n",
    "\n",
    "# FHIR resource → staging table mapping\n",
    "FHIR_STAGING_PRACT_MAP = {\n",
    "    \"Practitioner\": (\"practitioners_fhir_raw\", \"practitioner_id\"),\n",
    "    \"PractitionerRole\": (\"practitioner_roles_fhir_raw\", \"practitioner_role_id\")\n",
    "}\n",
    "# -----------------------------\n",
    "# DB config\n",
    "# -----------------------------\n",
    "DB_CONFIG = {\n",
    "    \"host\": \"localhost\",\n",
    "    \"port\": 5432,\n",
    "    \"database\": \"FHIR_staging\",\n",
    "    \"user\": \"postgres\",\n",
    "    \"password\": \"new_password\"\n",
    "}\n",
    "\n",
    "# ----------------------------\n",
    "# DB Connection\n",
    "# ----------------------------\n",
    "def get_db_connection():\n",
    "    return psycopg2.connect(\n",
    "        host=\"localhost\",\n",
    "        dbname=\"FHIR_staging\",\n",
    "        user=\"postgres\",\n",
    "        password=\"new_password\",\n",
    "        port=5432\n",
    "    )\n",
    "\n",
    "# ----------------------------\n",
    "# Batch insert\n",
    "# ----------------------------\n",
    "#df = pd.DataFrame()\n",
    "rows = []\n",
    "id_file_map = pd.DataFrame()\n",
    "\n",
    "def insert_practitioner_batch(conn, table, id_field, rows):\n",
    "    if not rows:\n",
    "        return 0\n",
    "\n",
    "    #sql = f\"\"\"\n",
    "    #    INSERT INTO fhir_staging.{table} ({id_field}, resource)\n",
    "    #    VALUES %s\n",
    "    #    ON CONFLICT ({id_field}) DO NOTHING\n",
    "    #\"\"\"\n",
    "    sql = f\"\"\"\n",
    "        INSERT INTO fhir_staging_sample.{table} ({id_field}, resource)\n",
    "        VALUES %s\n",
    "        ON CONFLICT ({id_field}) DO NOTHING\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with conn.cursor() as cur:\n",
    "            execute_values(cur, sql, rows)\n",
    "        return len(rows)\n",
    "    except Exception as e:\n",
    "        conn.rollback()\n",
    "        staging_logger.error(f\"Error inserting batch into {table}: {e}\")\n",
    "        return 0\n",
    "records =[]\n",
    "df = pd.DataFrame()\n",
    "# ----------------------------\n",
    "# Process a single hospital file\n",
    "# ----------------------------\n",
    "def process_practitioner_file(file_path):\n",
    "    count = 0\n",
    "    batch_map = defaultdict(list)\n",
    "\n",
    "    try:\n",
    "        conn = psycopg2.connect(**DB_CONFIG)\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            data = json.load(f)\n",
    "\n",
    "        if \"entry\" not in data:\n",
    "            staging_logger.warning(f\"No entries in {file_path}\")\n",
    "            return 0\n",
    "        \n",
    "        for entry in data[\"entry\"]:\n",
    "            resource = entry.get(\"resource\")\n",
    "            if not resource:\n",
    "                continue\n",
    "\n",
    "            rtype = resource.get(\"resourceType\")\n",
    "            rid = resource.get(\"id\")\n",
    "            records.append({\n",
    "                \"resource_type\" : rtype,\n",
    "                \"id\" : rid   \n",
    "            })\n",
    "            \n",
    "\n",
    "            if not rtype or not rid:\n",
    "                continue\n",
    "\n",
    "            if rtype not in FHIR_STAGING_PRACT_MAP:\n",
    "                staging_logger.warning(f\"Skipping unsupported resourceType: {rtype}\")\n",
    "                continue\n",
    "\n",
    "            table, id_field = FHIR_STAGING_PRACT_MAP[rtype]\n",
    "            batch_map[(table, id_field)].append((rid, Json(resource)))\n",
    "            count += 1\n",
    "\n",
    "        # Insert per table\n",
    "        for (table, id_field), rows in batch_map.items():\n",
    "            inserted = insert_practitioner_batch(conn, table, id_field, rows)\n",
    "            staging_logger.info(f\"Inserted {inserted} into {table}\")\n",
    "\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "\n",
    "        staging_logger.info(f\"Processed {count} resources from {file_path}\")\n",
    "        return count\n",
    "\n",
    "    except json.JSONDecodeError as e:\n",
    "        staging_logger.error(f\"Malformed JSON in {file_path}: {e}\")\n",
    "        return 0\n",
    "    except Exception as e:\n",
    "        staging_logger.error(f\"Error processing file {file_path}: {e}\")\n",
    "        return 0\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Sequential loader for all Hospital files\n",
    "# ----------------------------\n",
    "def load_practioner_files_sequential(practitioner_folder):\n",
    "    files = glob.glob(os.path.join(practitioner_folder, \"*.json\"))\n",
    "    if not files:\n",
    "        staging_logger.warning(f\"No Hospital JSON files found in {practitioner_folder}\")\n",
    "        return\n",
    "\n",
    "    total_resources = 0\n",
    "    for file_path in files:\n",
    "        count = process_practitioner_file(file_path)\n",
    "        total_resources += count\n",
    "\n",
    "    staging_logger.info(f\"Finished processing Practioner files. Total resources: {total_resources}\")\n",
    "\n",
    "# ----------------------------\n",
    "# Main execution\n",
    "# ----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    fhir_folder = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\"\n",
    "    practitioner_folder = os.path.join(fhir_folder, \"Practitioner\")\n",
    "    load_practioner_files_sequential(practitioner_folder)\n",
    "    df= pd.DataFrame(records)\n",
    "    df.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\pract2_file.csv\", index=False)\n",
    "    counts_df = df.groupby(\"resource_type\")[\"id\"].nunique().reset_index(name=\"unique_count\")\n",
    "    counts_df.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\count_Pract_role2_file.csv\")\n",
    "    #id_counts.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\count_ids_file.csv\")\n",
    "    #id_file_map = df.groupby(\"id\")[\"file\"].agg(list).reset_index()\n",
    "    #id_file_map.to_csv(r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\total_ids_file.csv\")\n",
    "    #file = r\"C:\\Users\\tonim\\Downloads\\output\\fhir\\Hospital\\hospitalInformation1755716738658.json\"\n",
    "    #load_hospital_files_sequential(file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
